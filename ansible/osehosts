# Create an OSEv3 group that contains the masters and nodes groups
[OSEv3:children]
masters
nodes
etcd
glusterfs

# Set variables common for all OSEv3 hosts
[OSEv3:vars]
# SSH user, this user should allow ssh based auth without requiring a password
ansible_ssh_user=root

# If ansible_ssh_user is not root, ansible_sudo must be set to true
#ansible_sudo=true

deployment_type=openshift-enterprise

# uncomment the following to enable htpasswd authentication; defaults to DenyAllPasswordIdentityProvider
openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '/etc/origin/master/htpasswd'}]


#Disable disk and memory checks
openshift_disable_check=memory_availability,disk_availability,docker_storage
#Set networking to multi-tenant
#os_sdn_network_plugin_name='redhat/openshift-ovs-multitenant'
#Disable SDN
#openshift_use_openshift_sdn=False
#openshift_set_hostname=True

#OCP 3.6 need use openshift_dns_ip since with vagrant/vbox we have 2 interfaces with DNS enp0s8 and enp0s3 and it seems that in OCP 3.6 installer having one interface NM_CONTROLLED="no" will impact DNSmasq config on each node so both interfaces need to have NM_CONTROLLED=yes
#https://bugzilla.redhat.com/show_bug.cgi?id=1481366
#due to VirtualBox enp0s3 on ip address 10.0.2.15
#using openshift_dns_ip on each node 
#openshift_dns_ip=10.100.192.201
#openshift_dns_ip=172.30.0.1
#Starting in 3.6 openshift_use_dnsmasq must be true
#openshift_use_dnsmasq=false

openshift_master_default_subdomain=cloudapps.example.com

openshift_registry_selector="region=infra"
openshift_router_selector="region=infra"

#Running etcd as an embedded service is no longer supported. If this is a new install please define an 'etcd' group with either one or three hosts. These hosts may be the same hosts as your masters. If this is an upgrade you may set openshift_master_unsupported_embedded_etcd=true until a migration playbook becomes available.
#openshift_master_unsupported_embedded_etcd=true

#openshift_hosted_metrics_deploy=true
#use_cluster_metrics=true
#openshift_master_metrics_public_url=https://hawkular-metrics.example.com/hawkular/metrics

# Configure nodeIP in the node config
# This is needed in cases where node traffic is desired to go over an
# interface other than the default network interface.
# openshift_node_set_node_ip=True

#Service Catalog
#openshift_enable_service_catalog=true
#openshift_template_service_broker_namespaces=['openshift','myproject']

# host group for masters
[masters]
ose-master.example.com

[etcd]
ose-master.example.com

# host group for nodes, includes region info
[nodes]
ose-master.example.com openshift_node_labels="{'region': 'infra', 'zone': 'master'}" openshift_ip=10.100.192.200 openshift_dns_ip=10.100.192.200 openshift_schedulable=true
ose-node-1.example.com openshift_node_labels="{'region': 'primary', 'zone': 'dev'}" openshift_ip=10.100.192.202 openshift_dns_ip=10.100.192.202
ose-node-2.example.com openshift_node_labels="{'region': 'primary', 'zone': 'test'}" openshift_ip=10.100.192.203 openshift_dns_ip=10.100.192.203
#ose-node-3.example.com openshift_node_labels="{'region': 'primary', 'zone': 'prod'}" openshift_ip=10.100.192.204 openshift_dns_ip=10.100.192.204

#wait for fix then uncomment
#https://bugzilla.redhat.com/show_bug.cgi?id=1423640
#https://bugzilla.redhat.com/show_bug.cgi?id=1473531
[glusterfs]
ose-node-1.example.com glusterfs_ip=10.100.192.202 glusterfs_devices='[ "/dev/sdb1" ]'
ose-node-2.example.com glusterfs_ip=10.100.192.203 glusterfs_devices='[ "/dev/sdb1" ]'
#ose-node-3.example.com glusterfs_ip=10.100.192.204 glusterfs_devices='[ "/dev/sdb1" ]'
ose-master.example.com glusterfs_ip=10.100.192.200 glusterfs_devices='[ "/dev/sdb1" ]'